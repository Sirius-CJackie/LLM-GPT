## Implement a ChatGPT-like LLM in PyTorch from scratch
# Implement text preprocessing, tokenization, embedding tensor 
Text Tokenization
Converting Tokens to Token IDs
Adding Special Context Tokens
Byte Pair Encoding 
Data Sampling with Sliding Window
Building the Embedding Layer
Positional Encoding


# Implement various attention mechanisms
Simple Attention Mechanism
Causal Attention Mechanism
Multi-head Causal Attention Mechanism

# Developed Transformer-based architecture
Forward
Normalization
Shortcut
Activation Layer
Linear Layer
Dropout
Text Generation

# Pretrained themodel on unlabeled text data set
Loss function(Cross entropy)
Training LLM
Decoding strategies to randomness of the generate 
Saving the trained-weights
Loading Open AI weights to model 


# Conducted classifier fine-tuning  
Preparing the classify Dataset and Dataloader
Adding a Classification Head
Calculating Classification Loss and Accuracy
Fine-tuning the Model with Supervised Data

# Conducted instruction fine-tuning
Creating Data Loaders for the Instruction Dataset
Fine-tuning the LLM with Instructions
Extracting and Saving Responses
Designed automated dialogue benchmark tests to evaluate model performance and conversational capabilities
